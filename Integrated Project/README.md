# Gold Recovery Prediction Project

This project analyzes and models the gold recovery process using industrial flotation data. It follows the structure of the TripleTen Integrated Project and includes data preparation, feature engineering, model training, evaluation, and final prediction generation through a multiâ€‘page Streamlit application.

---

## ğŸ“ Project Structure

```
project/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ gold_recovery_train.csv
â”‚   â”œâ”€â”€ gold_recovery_test.csv
â”‚   â””â”€â”€ gold_recovery_full.csv
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ modeling.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ analysis.py
â”‚
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ 1_Data_Overview.py
â”‚   â”œâ”€â”€ 2_Recovery_Validation.py
â”‚   â”œâ”€â”€ 3_Feature_Analysis.py
â”‚   â”œâ”€â”€ 4_Model_Training.py
â”‚   â””â”€â”€ 5_Final_Predictions.py
â”‚
â””â”€â”€ README.md
```

---

## ğŸ“¦ Installation

Create a virtual environment and install dependencies:

```bash
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

---

## ğŸš€ Running the Streamlit App

```bash
streamlit run 1_Data_Overview.py
```

Streamlit will automatically detect the `pages/` directory and expose the multiâ€‘page interface.

---

## ğŸ§¹ Data Preparation

The training dataset contains:

- Input features  
- Intermediate cleaner/rougher outputs  
- Final concentrate/tail outputs  
- Calculation columns  
- Target recovery values  

The test dataset contains **only input features**, so the preprocessing pipeline must:

- Drop all output and calculation columns from training  
- Align train/test columns using strict intersection  
- Build a numeric preprocessing pipeline (median imputation + scaling)

Example (from `preprocessing.py`):

```python
cols_to_drop = [
    c for c in df.columns
    if ".output." in c or ".calculation." in c or c in [TARGET_R, TARGET_F]
]
X_train_full = df.drop(columns=cols_to_drop)
```

---

## ğŸ§ª Recovery Formula Validation

The project includes a validation step comparing the provided recovery values with the computed formula:

```python
recovery = C * (F - T) / (F * (C - T))
```

Implemented in `metrics.py` as:

```python
def compute_recovery(feed, conc, tail, df):
    F = df[feed].astype(float)
    C = df[conc].astype(float)
    T = df[tail].astype(float)
    return np.where(F * (C - T) != 0, C * (F - T) / (F * (C - T)), np.nan)
```

---

## ğŸ¤– Model Training

Two models are trained for each target:

- **RandomForestRegressor**
- **LinearRegression**

Each wrapped in a pipeline:

```python
Pipeline([
    ("prep", preprocessor),
    ("model", RandomForestRegressor(...))
])
```

Evaluation metric:

- **SMAPE** (symmetric mean absolute percentage error)

Weighted score:

```
0.25 * rougher_smape + 0.75 * final_smape
```

The best model is selected automatically.

---

## ğŸ“ˆ Final Predictions

The final Streamlit page:

- Loads the best models  
- Applies them to the aligned test dataset  
- Outputs predicted rougher and final recovery values  
- Displays summary statistics and a preview table  

---

## ğŸ§  Key Lessons

- Train/test schema mismatch must be handled explicitly  
- Output and calculation columns must be removed from training  
- Preprocessing must be applied consistently through pipelines  
- SMAPE is sensitive to zero denominatorsâ€”handle with care  
- Streamlit multiâ€‘page apps benefit from modular utilities  

---

## ğŸ“œ License

This project is for educational and portfolio purposes.
