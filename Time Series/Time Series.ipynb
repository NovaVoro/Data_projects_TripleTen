{
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from typing import Tuple, Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAFE CSV LOADING\n",
    "# ============================================================\n",
    "def safe_read_csv(path: str, parse_dates: List[str] = None, index_col: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Safely read a CSV file with basic error handling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to the CSV file.\n",
    "    parse_dates : list of str, optional\n",
    "        Columns to parse as dates.\n",
    "    index_col : str, optional\n",
    "        Column to set as index.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        Loaded DataFrame.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        If the file does not exist.\n",
    "    ValueError\n",
    "        If the file cannot be parsed as CSV.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"File not found at path: {path}\")\n",
    "\n",
    "        df = pd.read_csv(path, parse_dates=parse_dates, index_col=index_col)\n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"[ERROR] {e}\")\n",
    "        raise\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"[ERROR] Failed to parse CSV file: {e}\")\n",
    "        raise ValueError(\"CSV parsing error\") from e\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Unexpected error while reading CSV: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TIME FEATURE ENGINEERING\n",
    "# ============================================================\n",
    "def create_time_features(df: pd.DataFrame, datetime_index: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create basic time-based features from a datetime index or column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame with a DatetimeIndex or a 'datetime' column.\n",
    "    datetime_index : bool, default True\n",
    "        If True, use the DataFrame index as the datetime source.\n",
    "        If False, use the 'datetime' column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with added time-based features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if datetime_index:\n",
    "            if not isinstance(df.index, pd.DatetimeIndex):\n",
    "                raise ValueError(\"DataFrame index must be a DatetimeIndex when datetime_index=True.\")\n",
    "            dt = df.index\n",
    "        else:\n",
    "            if \"datetime\" not in df.columns:\n",
    "                raise ValueError(\"Column 'datetime' not found for datetime_index=False.\")\n",
    "            dt = pd.to_datetime(df[\"datetime\"])\n",
    "\n",
    "        df = df.copy()\n",
    "        dt = df.index\n",
    "        df[\"hour\"] = dt.hour\n",
    "        df[\"dayofweek\"] = dt.dayofweek\n",
    "        df[\"day\"] = dt.day\n",
    "        df[\"month\"] = dt.month\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to create time features: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LAG FEATURE ENGINEERING\n",
    "# ============================================================\n",
    "def create_lag_features(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    lags: List[int],\n",
    "    rolling_windows: List[int]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create lag and rolling mean features for a time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame with a time index and target column.\n",
    "    target_col : str\n",
    "        Name of the target column.\n",
    "    lags : list of int\n",
    "        List of lag steps to create.\n",
    "    rolling_windows : list of int\n",
    "        List of window sizes for rolling mean features.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with added lag and rolling features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = df.copy()\n",
    "\n",
    "        # Create lag features\n",
    "        for lag in lags:\n",
    "            df[f\"lag_{lag}\"] = df[target_col].shift(lag)\n",
    "\n",
    "        # Create rolling mean features\n",
    "        for w in rolling_windows:\n",
    "            df[f\"rolling_mean_{w}\"] = (\n",
    "                df[target_col]\n",
    "                .shift(1)  # shift to avoid leakage of current target\n",
    "                .rolling(window=w, min_periods=1)\n",
    "                .mean()\n",
    "            )\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to create lag/rolling features: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN/TEST SPLIT\n",
    "# ============================================================\n",
    "def train_test_split_time_series(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    test_size_ratio=0.10\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Split a time series DataFrame into train and test sets based on a ratio.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame with time-ordered rows.\n",
    "    test_size_ratio : float\n",
    "        Fraction of data to use as test set (e.g., 0.1 for 10%).\n",
    "    target_col : str\n",
    "        Name of the target column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train : pd.DataFrame\n",
    "        Training features.\n",
    "    X_test : pd.DataFrame\n",
    "        Test features.\n",
    "    y_train : pd.Series\n",
    "        Training target.\n",
    "    y_test : pd.Series\n",
    "        Test target.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not 0 < test_size_ratio < 1:\n",
    "            raise ValueError(\"test_size_ratio must be between 0 and 1.\")\n",
    "\n",
    "        df = df.sort_index()\n",
    "        n_samples = len(df)\n",
    "        test_size = int(np.floor(n_samples * test_size_ratio))\n",
    "\n",
    "        if test_size == 0:\n",
    "            raise ValueError(\"Test size is zero; increase test_size_ratio or use a larger dataset.\")\n",
    "\n",
    "        train_df = df.iloc[:-test_size]\n",
    "        test_df = df.iloc[-test_size:]\n",
    "\n",
    "        X_train = train_df.drop(columns=[target_col])\n",
    "        y_train = train_df[target_col]\n",
    "        X_test = test_df.drop(columns=[target_col])\n",
    "        y_test = test_df[target_col]\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to split time series data: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL EVALUATION (WITH LOG INVERSION)\n",
    "# ============================================================\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name=\"model\"):\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict in log-space\n",
    "        y_pred_train_log = model.predict(X_train)\n",
    "        y_pred_test_log = model.predict(X_test)\n",
    "\n",
    "        # Invert transform\n",
    "        y_pred_train = np.expm1(y_pred_train_log)\n",
    "        y_pred_test = np.expm1(y_pred_test_log)\n",
    "\n",
    "        # Invert true values\n",
    "        y_train_orig = np.expm1(y_train)\n",
    "        y_test_orig = np.expm1(y_test)\n",
    "\n",
    "        # RMSE in original units\n",
    "        train_rmse = mean_squared_error(y_train_orig, y_pred_train, squared=False)\n",
    "        test_rmse = mean_squared_error(y_test_orig, y_pred_test, squared=False)\n",
    "\n",
    "        print(f\"\\n[{model_name}]\")\n",
    "        print(f\"  Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"  Test  RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "        return {\n",
    "            \"model\": model,\n",
    "            \"model_name\": model_name,\n",
    "            \"train_rmse\": train_rmse,\n",
    "            \"test_rmse\": test_rmse,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to evaluate model '{model_name}': {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(df: pd.DataFrame, target_col: str, title: str = \"Time Series\"):\n",
    "    \"\"\"\n",
    "    Plot a time series for quick visual inspection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame with a DatetimeIndex.\n",
    "    target_col : str\n",
    "        Name of the target column.\n",
    "    title : str, default \"Time Series\"\n",
    "        Plot title.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        df[target_col].plot()\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(target_col)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to plot time series: {e}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model, feature_names: List[str], top_n: int = 20, title: str = \"Feature Importances\"):\n",
    "    \"\"\"\n",
    "    Plot feature importances for tree-based models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : estimator\n",
    "        A fitted model with feature_importances_ attribute.\n",
    "    feature_names : list of str\n",
    "        Names of the features.\n",
    "    top_n : int, default 20\n",
    "        Number of top features to display.\n",
    "    title : str, default \"Feature Importances\"\n",
    "        Plot title.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not hasattr(model, \"feature_importances_\"):\n",
    "            print(\"[INFO] Model has no 'feature_importances_' attribute; skipping feature importance plot.\")\n",
    "            return\n",
    "\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1][:top_n]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(\n",
    "            x=importances[indices],\n",
    "            y=np.array(feature_names)[indices],\n",
    "            orient=\"h\"\n",
    "        )\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to plot feature importances: {e}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     num_orders\n",
      "datetime                       \n",
      "2018-03-01 00:00:00           9\n",
      "2018-03-01 00:10:00          14\n",
      "2018-03-01 00:20:00          28\n",
      "2018-03-01 00:30:00          20\n",
      "2018-03-01 00:40:00          32\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 26496 entries, 2018-03-01 00:00:00 to 2018-08-31 23:50:00\n",
      "Data columns (total 1 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   num_orders  26496 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 414.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# LOAD RAW DATA\n",
    "# -----------------------------\n",
    "data_path = \"/datasets/taxi.csv\"\n",
    "target_col = \"num_orders\"\n",
    "\n",
    "df = safe_read_csv(\n",
    "    path=data_path,\n",
    "    parse_dates=[\"datetime\"],\n",
    "    index_col=\"datetime\"\n",
    ")\n",
    "\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     num_orders\n",
      "datetime                       \n",
      "2018-03-01 00:00:00         124\n",
      "2018-03-01 01:00:00          85\n",
      "2018-03-01 02:00:00          71\n",
      "2018-03-01 03:00:00          66\n",
      "2018-03-01 04:00:00          43\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# RESAMPLE TO HOURLY\n",
    "# -----------------------------\n",
    "raw_target = \"num_orders\"\n",
    "df_hourly = df.resample(\"1H\").sum()\n",
    "df_hourly[raw_target] = df_hourly[raw_target].fillna(0)\n",
    "\n",
    "print(df_hourly.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOG TRANSFORM TARGET\n",
    "# ============================================================\n",
    "df_hourly[\"log_orders\"] = np.log1p(df_hourly[raw_target])\n",
    "target_col = \"log_orders\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     hour  dayofweek  day  month     lag_1     lag_2  \\\n",
      "datetime                                                               \n",
      "2018-03-03 00:00:00     0          5    3      3  4.290459  4.787492   \n",
      "2018-03-03 01:00:00     1          5    3      3  5.099866  4.290459   \n",
      "2018-03-03 02:00:00     2          5    3      3  4.691348  5.099866   \n",
      "2018-03-03 03:00:00     3          5    3      3  3.258097  4.691348   \n",
      "2018-03-03 04:00:00     4          5    3      3  4.174387  3.258097   \n",
      "\n",
      "                        lag_3    lag_24    lag_48  rolling_mean_3  \\\n",
      "datetime                                                            \n",
      "2018-03-03 00:00:00  4.624973  4.510860  4.828314        4.567641   \n",
      "2018-03-03 01:00:00  4.787492  4.795791  4.454347        4.725939   \n",
      "2018-03-03 02:00:00  4.290459  4.330733  4.276666        4.693891   \n",
      "2018-03-03 03:00:00  5.099866  4.174387  4.204693        4.349770   \n",
      "2018-03-03 04:00:00  4.691348  3.044522  3.784190        4.041277   \n",
      "\n",
      "                     rolling_mean_6  rolling_mean_12  rolling_mean_24  \\\n",
      "datetime                                                                \n",
      "2018-03-03 00:00:00        4.443009         4.237356         3.956739   \n",
      "2018-03-03 01:00:00        4.543018         4.361435         3.981281   \n",
      "2018-03-03 02:00:00        4.672906         4.433328         3.976929   \n",
      "2018-03-03 03:00:00        4.458706         4.387614         3.932236   \n",
      "2018-03-03 04:00:00        4.383608         4.394284         3.932236   \n",
      "\n",
      "                     log_orders  \n",
      "datetime                         \n",
      "2018-03-03 00:00:00    5.099866  \n",
      "2018-03-03 01:00:00    4.691348  \n",
      "2018-03-03 02:00:00    3.258097  \n",
      "2018-03-03 03:00:00    4.174387  \n",
      "2018-03-03 04:00:00    3.806662  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ============================================================\n",
    "df_features = create_time_features(df_hourly)\n",
    "\n",
    "df_features = create_lag_features(\n",
    "    df_features,\n",
    "    target_col=target_col,\n",
    "    lags=[1, 2, 3, 24, 48],\n",
    "    rolling_windows=[3, 6, 12, 24]\n",
    ")\n",
    "\n",
    "df_features = df_features.dropna()\n",
    "\n",
    "# REMOVE BOTH TARGETS TO PREVENT LEAKAGE\n",
    "df_features = df_features.drop(columns=[\"num_orders\", \"log_orders\"], errors=\"ignore\")\n",
    "\n",
    "# ADD BACK ONLY THE TARGET YOU WANT TO PREDICT\n",
    "df_features[target_col] = df_hourly[target_col]\n",
    "\n",
    "print(df_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    4416.000000\n",
      "mean        4.286042\n",
      "std         0.634665\n",
      "min         0.000000\n",
      "25%         4.007333\n",
      "50%         4.369448\n",
      "75%         4.682131\n",
      "max         6.137727\n",
      "Name: log_orders, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# BASIC STATISTICS\n",
    "# -----------------------------\n",
    "print(df_hourly[target_col].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3932, 13) (436, 13)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split_time_series(\n",
    "    df_features,\n",
    "    target_col=target_col,\n",
    "    test_size_ratio=0.10\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Linear Regression]\n",
      "  Train RMSE: 24.772\n",
      "  Test  RMSE: 43.030\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# LINEAR REGRESSION\n",
    "# -----------------------------\n",
    "lr_model = Pipeline([\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"lr\", LinearRegression())\n",
    "])\n",
    "\n",
    "lr_results = evaluate_model(\n",
    "    lr_model, X_train, y_train, X_test, y_test, \"Linear Regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Random Forest]\n",
      "  Train RMSE: 16.760\n",
      "  Test  RMSE: 43.963\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# RANDOM FOREST (GRID SEARCH)\n",
    "# -----------------------------\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "rf_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [10, 20, None],\n",
    "    \"min_samples_split\": [2, 5]\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "rf_search = GridSearchCV(\n",
    "    rf,\n",
    "    rf_grid,\n",
    "    cv=tscv,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_search.fit(X_train, y_train)\n",
    "best_rf = rf_search.best_estimator_\n",
    "\n",
    "rf_results = evaluate_model(\n",
    "    best_rf, X_train, y_train, X_test, y_test, \"Random Forest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Gradient Boosting]\n",
      "  Train RMSE: 21.114\n",
      "  Test  RMSE: 43.469\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# GRADIENT BOOSTING (GRID SEARCH)\n",
    "# -----------------------------\n",
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "gb_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"max_depth\": [3, 5]\n",
    "}\n",
    "\n",
    "gb_search = GridSearchCV(\n",
    "    gb,\n",
    "    gb_grid,\n",
    "    cv=tscv,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gb_search.fit(X_train, y_train)\n",
    "best_gb = gb_search.best_estimator_\n",
    "\n",
    "gb_results = evaluate_model(\n",
    "    best_gb, X_train, y_train, X_test, y_test, \"Gradient Boosting\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "FINAL MODEL RESULTS\n",
      "======================\n",
      "Linear Regression: Test RMSE = 43.030\n",
      "Random Forest: Test RMSE = 43.963\n",
      "Gradient Boosting: Test RMSE = 43.469\n",
      "\n",
      "======================\n",
      "BEST MODEL\n",
      "======================\n",
      "Best Model: Linear Regression\n",
      "Best RMSE : 43.030\n",
      "\n",
      "SUCCESS: RMSE requirement met.\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"Linear Regression\": lr_results[\"test_rmse\"],\n",
    "    \"Random Forest\": rf_results[\"test_rmse\"],\n",
    "    \"Gradient Boosting\": gb_results[\"test_rmse\"]\n",
    "}\n",
    "\n",
    "print(\"\\n======================\")\n",
    "print(\"FINAL MODEL RESULTS\")\n",
    "print(\"======================\")\n",
    "\n",
    "for name, rmse in results.items():\n",
    "    print(f\"{name}: Test RMSE = {rmse:.3f}\")\n",
    "\n",
    "best_model = min(results, key=results.get)\n",
    "best_rmse = results[best_model]\n",
    "\n",
    "print(\"\\n======================\")\n",
    "print(\"BEST MODEL\")\n",
    "print(\"======================\")\n",
    "print(f\"Best Model: {best_model}\")\n",
    "print(f\"Best RMSE : {best_rmse:.3f}\")\n",
    "\n",
    "if best_rmse <= 48:\n",
    "    print(\"\\nSUCCESS: RMSE requirement met.\")\n",
    "else:\n",
    "    print(\"\\nWARNING: RMSE requirement NOT met. Further tuning recommended.\")"
   ]
  },
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
